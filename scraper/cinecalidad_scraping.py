import requests
from bs4 import BeautifulSoup
import json
import time

class CinecalidadScraper:
    def __init__(self):
        self.base_url = "https://cinecalidad.bar"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def extraer_peliculas(self, url=None, pagina=1, tipo='pelicula'):
        """
        Extrae informaci√≥n de las pel√≠culas de la p√°gina principal o una p√°gina espec√≠fica
        """
        if url is None:
            if pagina == 1:
                url = self.base_url
            else:
                url = f"{self.base_url}/page/{pagina}/"
        
        try:
            print(f"Scrapeando: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Encontrar todos los art√≠culos de pel√≠culas
            peliculas = soup.find_all('article', class_='tposty')
            
            datos_peliculas = []
            
            for pelicula in peliculas:
                try:
                    # Extraer el enlace
                    enlace_tag = pelicula.find('a', class_='absolute')
                    enlace = enlace_tag['href'] if enlace_tag else None
                    
                    # Extraer el t√≠tulo
                    titulo_span = pelicula.find('span', class_='sr-only')
                    titulo = titulo_span.text.strip() if titulo_span else None
                    
                    # Extraer la imagen
                    img_tag = pelicula.find('img')
                    imagen = img_tag['src'] if img_tag else None
                    
                    # Extraer calidad y a√±o
                    calidad_tag = pelicula.find('span', class_='quality')
                    calidad = calidad_tag.text.strip() if calidad_tag else None
                    
                    a√±o_tag = pelicula.find('span', class_='year')
                    a√±o = a√±o_tag.text.strip() if a√±o_tag else None
                    
                    # Extraer descripci√≥n
                    desc_tag = pelicula.find('p', class_='text-sm opacity-70')
                    descripcion = desc_tag.text.strip() if desc_tag else None
                    
                    # Extraer g√©neros
                    generos_container = pelicula.find('p', class_=['absolute', 'bottom-0'])
                    generos = []
                    if generos_container:
                        generos_links = generos_container.find_all('a')
                        generos = [g.text.strip() for g in generos_links]
                    
                    pelicula_data = {
                        'tipo': tipo,
                        'titulo': titulo,
                        'enlace': enlace,
                        'imagen': imagen,
                        'calidad': calidad,
                        'a√±o': a√±o,
                        'descripcion': descripcion,
                        'generos': generos if generos else []
                    }
                    
                    datos_peliculas.append(pelicula_data)
                    
                except Exception as e:
                    print(f"Error al extraer pel√≠cula: {e}")
                    continue
            
            print(f"‚úì {len(datos_peliculas)} pel√≠culas extra√≠das de la p√°gina {pagina}")
            return datos_peliculas
            
        except Exception as e:
            print(f"Error al hacer scraping: {e}")
            return []
        
    def extraer_series(self, url=None, pagina=1, tipo='serie'):
        """
        Extrae informaci√≥n de las series de la p√°gina principal o una p√°gina espec√≠fica
        """
        if url is None:
            if pagina == 1:
                url = f"{self.base_url}/serie/"
            else:
                url = f"{self.base_url}/serie/page/{pagina}/"
    
        try:
            print(f"Scrapeando: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
        
            soup = BeautifulSoup(response.content, 'html.parser')
        
            # Encontrar todos los art√≠culos de series
            series = soup.find_all('article', class_='tposty')
        
            datos_series = []
        
            for serie in series:
                try:                    
                    # Extraer informaci√≥n de temporadas y episodios
                    temporadas_tag = serie.find('span', class_='last-s')
                    temporadas = temporadas_tag.text.strip() if temporadas_tag else None
                    
                    episodios_tag = serie.find('span', class_='last-ep')
                    episodios = episodios_tag.text.strip() if episodios_tag else None
                    
                    # Extraer el enlace
                    enlace_tag = serie.find('a', class_='absolute')
                    enlace = enlace_tag['href'] if enlace_tag else None
                
                    # Extraer el t√≠tulo
                    titulo_span = serie.find('span', class_='sr-only')
                    titulo = titulo_span.text.strip() if titulo_span else None
                
                    # Extraer la imagen
                    img_tag = serie.find('img')
                    imagen = img_tag['src'] if img_tag else None
                
                    # Extraer calidad y a√±o
                    calidad_tag = serie.find('span', class_='quality')
                    calidad = calidad_tag.text.strip() if calidad_tag else None
                
                    a√±o_tag = serie.find('span', class_='year')
                    a√±o = a√±o_tag.text.strip() if a√±o_tag else None
                
                    # Extraer descripci√≥n
                    desc_tag = serie.find('p', class_='text-sm opacity-70')
                    descripcion = desc_tag.text.strip() if desc_tag else None
                
                    # Extraer g√©neros
                    generos_container = serie.find('p', class_=['absolute', 'bottom-0'])
                    generos = []
                    if generos_container:
                        generos_links = generos_container.find_all('a')
                        generos = [g.text.strip() for g in generos_links]
                
                    serie_data = {
                        'tipo': tipo,
                        'titulo': titulo,
                        'enlace': enlace,
                        'imagen': imagen,
                        'temporadas': temporadas,
                        'episodios': episodios,
                        'calidad': calidad,
                        'a√±o': a√±o,
                        'descripcion': descripcion,
                        'generos': generos if generos else []
                    }
                
                    datos_series.append(serie_data)
                
                except Exception as e:
                    print(f"Error al extraer serie: {e}")
                    continue
        
            print(f"‚úì {len(datos_series)} series extra√≠das de la p√°gina {pagina}")
            return datos_series
        
        except Exception as e:
            print(f"Error al hacer scraping: {e}")
            return []
    
    def extraer_multiples_paginas(self, num_paginas=3, tipo='pelicula'):
        """
        Extrae pel√≠culas o series de m√∫ltiples p√°ginas
        tipo: 'pelicula' o 'serie'
        """
        todos_items = []        
        for pagina in range(1, num_paginas + 1):
            print(f"\n{'='*60}")
            print(f"P√°gina {pagina} de {num_paginas} - {tipo_texto}")
            print('='*60)

            if tipo == 'serie':
                items = self.extraer_series(pagina=pagina, tipo=tipo)
            else:
                items = self.extraer_peliculas(pagina=pagina, tipo=tipo)

            todos_items.extend(items)
            
            # Pausa entre p√°ginas para ser respetuoso
            if pagina < num_paginas:
                time.sleep(2)
        
        return todos_items
    
    def guardar_json(self, datos, archivo='peliculas_cinecalidad.json'):
        """
        Guarda los datos en un archivo JSON
        """
        if not datos:
            print("No hay datos para guardar")
            return
        
        with open(archivo, 'w', encoding='utf-8') as f:
            json.dump(datos, f, ensure_ascii=False, indent=2)
        
        print(f"‚úì Datos guardados en '{archivo}'")
    
    def mostrar_peliculas(self, datos, limite=5):
        """
        Muestra las pel√≠culas o series extra√≠das en la consola
        """
        print(f"\n{'='*80}")
        print(f"ITEMS ENCONTRADOS (Mostrando {min(limite, len(datos))} de {len(datos)})")
        print('='*80)
        
        for i, item in enumerate(datos[:limite], 1):
            tipo_emoji = "üì∫" if item.get('tipo') == 'serie' else "üé¨"
            print(f"\n{i}. {tipo_emoji} {item['titulo']}")
            
            if item.get('tipo') == 'serie':
                print(f"   {item.get('temporadas', 'N/A')} | {item.get('episodios', 'N/A')}")
            else:
                print(f"   A√±o: {item['a√±o']} | Calidad: {item['calidad']}")
            
            print(f"   G√©neros: {', '.join(item['generos']) if item['generos'] else 'N/A'}")
            print(f"   URL: {item['enlace']}")
            
            if item['descripcion']:
                desc = item['descripcion'][:100] + "..." if len(item['descripcion']) > 100 else item['descripcion']
                print(f"   Descripci√≥n: {desc}")

    def obtener_numero_paginas(self, tipo):
        """
        Obtiene el n√∫mero total de p√°ginas disponibles
        
        Args:
            tipo (str): 'peliculas' o 'series'
        
        Returns:
            int: N√∫mero total de p√°ginas, o 1 si hay error
        """
        if tipo == 'pelicula':
            url = self.base_url
        elif tipo == 'serie':
            url = f"{self.base_url}/serie/"
        else:
            print("Tipo no v√°lido. Usa 'peliculas' o 'series'")
            return 1
        
        try:
            print(f"Obteniendo n√∫mero de p√°ginas de {tipo}...")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Buscar el contenedor de paginaci√≥n
            nav_pagination = soup.find('nav', class_='navigation pagination')
            
            if not nav_pagination:
                print("No se encontr√≥ paginaci√≥n")
                return 1
            
            # Buscar todos los enlaces de p√°gina
            page_links = nav_pagination.find_all('a', class_='page-numbers')
            
            # Filtrar solo los enlaces num√©ricos (excluir "Siguiente")
            numeros_pagina = []
            for link in page_links:
                texto = link.text.strip()
                if texto.isdigit():
                    numeros_pagina.append(int(texto))
            
            # El n√∫mero m√°s alto es el total de p√°ginas
            if numeros_pagina:
                total_paginas = max(numeros_pagina)
                print(f"‚úì Total de p√°ginas encontradas: {total_paginas}")
                return total_paginas
            else:
                print("No se encontraron n√∫meros de p√°gina")
                return 1
                
        except Exception as e:
            print(f"Error al obtener n√∫mero de p√°ginas: {e}")
            return 1

# Ejemplo de uso
if __name__ == "__main__":
    scraper = CinecalidadScraper()

    print("üé¨ SCRAPER DE CINECALIDAD")
    print("=" * 80)

    tipo = input("¬øQu√© deseas extraer? Pel√≠culas o Series (p/s): ").strip().lower()
    if tipo not in ['p', 's']:
        print("‚ö†Ô∏è Opci√≥n inv√°lida. Se usar√° 'pel√≠cula' por defecto.")
        tipo = 'p'

    tipo_texto = "serie" if tipo == 's' else "pelicula"
    print(f"\nüìÇ Tipo de contenido seleccionado: {tipo_texto.upper()}")

    total_paginas = scraper.obtener_numero_paginas(tipo=tipo_texto)

    print("\n" + "=" * 80)
    multiple = input("¬øDeseas extraer de m√∫ltiples p√°ginas? (s/n): ").strip().lower()

    if multiple == 's':
        try:
            num_paginas = int(input(f"¬øCu√°ntas p√°ginas quieres scrapear? (1-{total_paginas}): "))
            if not (1 <= num_paginas <= total_paginas):
                print(f"‚ö†Ô∏è El n√∫mero de p√°ginas debe estar entre 1 y {total_paginas}.")
                exit()

            print(f"\nüìö Extrayendo {tipo_texto}s de {num_paginas} p√°ginas...")
            resultados = scraper.extraer_multiples_paginas(num_paginas=num_paginas, tipo=tipo_texto)

            scraper.mostrar_peliculas(resultados, limite=15)
            archivo = f"{tipo_texto}s_{num_paginas}_paginas.json"
            scraper.guardar_json(resultados, archivo)
            print(f"\nüìä Total de {tipo_texto}s extra√≠das: {len(resultados)}")

        except ValueError:
            print("‚ö†Ô∏è Debes ingresar un n√∫mero v√°lido para la cantidad de p√°ginas.")

    # --- MODO UNA SOLA P√ÅGINA ---
    else:
        print(f"\nüìÑ Extrayendo {tipo_texto}s de la primera p√°gina...")
        resultados = scraper.extraer_multiples_paginas(num_paginas=1, tipo=tipo_texto)
        scraper.mostrar_peliculas(resultados, limite=10)
        archivo = f"{tipo_texto}s_pagina_1.json"
        scraper.guardar_json(resultados, archivo)

    print("\n‚úÖ Scraping completado exitosamente!")